name: 📊 Performance Tracking (Informational)

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'datason/**'
      - 'benchmarks/**'
      - 'pyproject.toml'
  schedule:
    # Run weekly to track performance over time
    - cron: '0 8 * * 1'  # Every Monday at 8 AM UTC
  workflow_dispatch:
    inputs:
      save_baseline:
        description: 'Save current results as new baseline'
        required: false
        default: false
        type: boolean
      regression_threshold:
        description: 'Regression threshold percentage (default: 25)'
        required: false
        default: '25'
        type: string

permissions:
  contents: read
  pages: write
  id-token: write

env:
  PYTHONUNBUFFERED: 1
  # Environment-aware threshold - CI environments are more variable
  PERFORMANCE_REGRESSION_THRESHOLD: ${{ github.event.inputs.regression_threshold || '25' }}

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: 💾 Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: performance-${{ runner.os }}-py3.11-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          performance-${{ runner.os }}-py3.11-

    - name: 📦 Install package and dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest  # Minimal dependencies for performance testing

    - name: 📂 Create results directory
      run: mkdir -p benchmarks/results

    - name: 📥 Download previous CI baseline (if available)
      uses: actions/cache@v4
      with:
        path: benchmarks/results
        key: performance-baseline-ci-${{ runner.os }}-${{ github.repository }}
        restore-keys: |
          performance-baseline-ci-${{ runner.os }}-

    - name: 🚀 Run performance benchmarks
      run: |
        cd benchmarks
        # Set environment-aware threshold
        export PERFORMANCE_REGRESSION_THRESHOLD="${{ env.PERFORMANCE_REGRESSION_THRESHOLD }}"
        python ci_performance_tracker.py
      env:
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF: ${{ github.ref }}
        GITHUB_RUN_ID: ${{ github.run_id }}
        RUNNER_OS: ${{ runner.os }}
        CI_ENVIRONMENT: true

    - name: 📋 Save baseline (if requested or weekly)
      if: ${{ github.event.inputs.save_baseline == 'true' || (github.ref == 'refs/heads/main' && github.event_name == 'schedule') }}
      run: |
        cd benchmarks
        cp results/latest.json results/baseline_ci.json
        echo "✅ Saved new CI baseline"

    - name: 📊 Generate performance report
      run: |
        cd benchmarks
        python -c "
        import json
        import os

        # Load latest comparison
        if os.path.exists('results/latest_comparison.json'):
            with open('results/latest_comparison.json', 'r') as f:
                comparison = json.load(f)

            # Create GitHub Actions summary
            with open(os.environ['GITHUB_STEP_SUMMARY'], 'w') as f:
                f.write('# 📊 Performance Test Results (Informational)\n\n')
                f.write('> **Note**: Performance tests are informational and do not block CI due to environment variability.\n\n')

                if comparison['status'] == 'baseline_created':
                    f.write('✅ **Baseline created successfully**\n\n')
                    f.write('This is the first CI run, so results have been saved as the new CI baseline.\n')
                else:
                    f.write(f'📋 **Compared with baseline from:** {comparison[\"baseline_metadata\"].get(\"timestamp\", \"unknown\")}\n\n')

                    # Note about environment differences
                    if comparison.get('environment_mismatch'):
                        f.write('⚠️ **Environment Difference Detected**: Baseline from different environment, results may not be directly comparable.\n\n')

                    if comparison['regressions']:
                        f.write(f'## 🔴 Performance Regressions ({len(comparison[\"regressions\"])}) - Threshold: {os.environ.get(\"PERFORMANCE_REGRESSION_THRESHOLD\", \"25\")}%\n\n')
                        f.write('| Test | Change | Current | Baseline | Severity |\n')
                        f.write('|------|--------|---------|----------|----------|\n')
                        for reg in comparison['regressions']:
                            severity = '🔥 Major' if reg[\"change_pct\"] > 50 else '⚠️ Minor'
                            f.write(f'| {reg[\"test\"]} | {reg[\"change_pct\"]:+.1f}% | {reg[\"current_ms\"]:.2f}ms | {reg[\"baseline_ms\"]:.2f}ms | {severity} |\n')
                        f.write('\n')

                    if comparison['improvements']:
                        f.write(f'## 🟢 Performance Improvements ({len(comparison[\"improvements\"])})\n\n')
                        f.write('| Test | Change | Current | Baseline |\n')
                        f.write('|------|--------|---------|----------|\n')
                        for imp in comparison['improvements']:
                            f.write(f'| {imp[\"test\"]} | {imp[\"change_pct\"]:+.1f}% | {imp[\"current_ms\"]:.2f}ms | {imp[\"baseline_ms\"]:.2f}ms |\n')
                        f.write('\n')

                    if not comparison['regressions'] and not comparison['improvements']:
                        f.write('🟡 **No significant performance changes detected**\n\n')
                        f.write(f'All performance metrics are within {os.environ.get(\"PERFORMANCE_REGRESSION_THRESHOLD\", \"25\")}% of baseline values.\n')

                    # Add guidance for interpreting results
                    f.write('## 📝 Interpreting Results\n\n')
                    f.write('- **Environment Impact**: CI runners have variable performance; differences <50% may be environment noise\n')
                    f.write('- **Focus on Patterns**: Look for consistent regressions across multiple tests\n')
                    f.write('- **Major Regressions**: Changes >100% warrant investigation\n')
                    f.write('- **Micro-benchmarks**: Tests <1ms are especially susceptible to environment variation\n')
        "

    - name: 💾 Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results-${{ github.run_id }}
        path: |
          benchmarks/results/*.json
        retention-days: 90

    - name: 💾 Save CI baseline cache
      uses: actions/cache/save@v4
      if: github.ref == 'refs/heads/main'
      with:
        path: benchmarks/results
        key: performance-baseline-ci-${{ runner.os }}-${{ github.repository }}-${{ github.run_id }}

    - name: ℹ️ Performance Status (Informational Only)
      if: always()
      run: |
        cd benchmarks
        echo "📊 Performance tests completed successfully"
        echo "✅ Results are informational and do not block CI"

        if [ -f "results/latest_comparison.json" ]; then
          python -c "
          import json

          with open('results/latest_comparison.json', 'r') as f:
              comparison = json.load(f)

          if comparison.get('regressions'):
              major_regressions = [r for r in comparison['regressions'] if r['change_pct'] > 100]
              if major_regressions:
                  print(f'⚠️ Notice: {len(major_regressions)} major performance regressions detected (>100%)')
                  print('   Consider investigating if this was expected')
              else:
                  print(f'ℹ️ Info: {len(comparison[\"regressions\"])} minor performance regressions detected')
                  print('   Likely due to environment differences')
          else:
              print('✅ No significant performance regressions detected')
          "
        fi

        # Always exit 0 - performance tests are informational
        exit 0

  # Optional: Generate historical performance charts
  generate-charts:
    runs-on: ubuntu-latest
    needs: performance-test
    if: github.ref == 'refs/heads/main'

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: 📦 Install chart dependencies
      run: |
        pip install matplotlib seaborn pandas

    - name: 📥 Download performance results
      uses: actions/download-artifact@v4
      with:
        name: performance-results-${{ github.run_id }}
        path: benchmarks/results

    - name: 📊 Generate performance charts
      run: |
        python scripts/generate_performance_chart.py

    - name: 📊 Upload performance chart
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-chart-${{ github.run_id }}
        path: benchmarks/performance_chart.png
        retention-days: 30
